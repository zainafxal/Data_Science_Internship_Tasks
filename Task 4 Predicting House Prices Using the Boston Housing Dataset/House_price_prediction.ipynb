{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462b54c4",
   "metadata": {},
   "source": [
    "# Predicting House Prices Using the Boston Housing Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f1842",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "This Jupyter Notebook demonstrates building regression models (Linear Regression, Random Forest, and conceptually XGBoost) from scratch to predict house prices using the Boston Housing Dataset. We will cover data preprocessing, custom model implementations, performance comparison using RMSE and R², and visualization of feature importance for tree-based models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d5a35",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "**Dataset Used:** Boston Housing Dataset (fetched from [OpenML](https://www.openml.org/d/531))\n",
    "\n",
    "**Source:**  \n",
    "The dataset is retrieved from OpenML using the `fetch_openml()` function from `sklearn.datasets`. It is commonly used for regression problems in machine learning and includes various features of residential homes in Boston.\n",
    "\n",
    "**Dataset Summary:**  \n",
    "- **Instances:** 506  \n",
    "- **Features:** 13 numerical predictors  \n",
    "- **Target Variable:** `MEDV` (Median value of owner-occupied homes in $1000s)\n",
    "\n",
    "**Features Description:**\n",
    "- `CRIM`: Per capita crime rate by town  \n",
    "- `ZN`: Proportion of residential land zoned for lots over 25,000 sq.ft.  \n",
    "- `INDUS`: Proportion of non-retail business acres per town  \n",
    "- `CHAS`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  \n",
    "- `NOX`: Nitrogen oxides concentration (parts per 10 million)  \n",
    "- `RM`: Average number of rooms per dwelling  \n",
    "- `AGE`: Proportion of owner-occupied units built prior to 1940  \n",
    "- `DIS`: Weighted distances to five Boston employment centres  \n",
    "- `RAD`: Index of accessibility to radial highways  \n",
    "- `TAX`: Full-value property-tax rate per $10,000  \n",
    "- `PTRATIO`: Pupil-teacher ratio by town  \n",
    "- `B`: 1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town  \n",
    "- `LSTAT`: Percentage lower status of the population  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c097d6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Boston Housing Dataset\n",
    "\n",
    "\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "df = boston.frame\n",
    "X = df.drop('MEDV', axis=1)\n",
    "y = df['MEDV']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled data back to Pandas DataFrames for easier handling (optional)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554990a",
   "metadata": {},
   "source": [
    "## Model Implementation (From Scratch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95998a20",
   "metadata": {},
   "source": [
    "### 1. Linear Regression (From Scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becbf5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, n_iterations=1000):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "            # Calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24c4fe",
   "metadata": {},
   "source": [
    "### 2. Random Forest (Conceptual Outline & Simplified Implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df156dbb",
   "metadata": {},
   "source": [
    "Implementing a full Random Forest from scratch involves building multiple decision trees, each trained on a random subset of the data and a random subset of the features. The final prediction is the average of the predictions from all the trees.\n",
    "\n",
    "Due to complexity, here's a simplified structure to illustrate the concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59491145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressorScratch:\n",
    "    def __init__(self, min_samples_split=2, max_depth=None, random_feature_subset=None):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.random_feature_subset = random_feature_subset\n",
    "        self.tree = None\n",
    "\n",
    "    def _split(self, X, y, feature_indices):\n",
    "        best_split = {}\n",
    "        return best_split\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            return np.mean(y)\n",
    "\n",
    "        if self.random_feature_subset is not None:\n",
    "            feature_indices = np.random.choice(n_features, self.random_feature_subset, replace=False)\n",
    "        else:\n",
    "            feature_indices = np.arange(n_features)\n",
    "\n",
    "        best_split = self._split(X, y, feature_indices)\n",
    "        if not best_split:\n",
    "            return np.mean(y)\n",
    "\n",
    "        return {'feature_index': best_split['feature_index'], 'threshold': best_split['threshold'], 'left': ..., 'right': ...}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if isinstance(node, (int, float)):\n",
    "            return node\n",
    "        if x[node['feature_index']] <= node['threshold']:\n",
    "            return self._traverse_tree(x, node['left'])\n",
    "        return self._traverse_tree(x, node['right'])\n",
    "\n",
    "class RandomForestRegressorScratch:\n",
    "    def __init__(self, n_estimators=10, min_samples_split=2, max_depth=None, random_feature_subset='sqrt'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.random_feature_subset = random_feature_subset\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        for _ in range(self.n_estimators):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot, y_boot = X[indices], y[indices]\n",
    "\n",
    "            if self.random_feature_subset == 'sqrt':\n",
    "                n_feat_subset = int(np.sqrt(n_features))\n",
    "            elif isinstance(self.random_feature_subset, int):\n",
    "                n_feat_subset = self.random_feature_subset\n",
    "            else:\n",
    "                n_feat_subset = n_features\n",
    "\n",
    "            tree = DecisionTreeRegressorScratch(min_samples_split=self.min_samples_split,\n",
    "                                                max_depth=self.max_depth,\n",
    "                                                random_feature_subset=n_feat_subset)\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91abfe8",
   "metadata": {},
   "source": [
    "## Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24e1829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (From Scratch) Performance:\n",
      "RMSE: 4.9917\n",
      "R²: 0.6602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "lr_scratch = LinearRegressionScratch()\n",
    "lr_scratch.fit(X_train_scaled, y_train, learning_rate=0.01, n_iterations=1500)\n",
    "y_pred_lr_scratch = lr_scratch.predict(X_test_scaled)\n",
    "\n",
    "rmse_lr_scratch = np.sqrt(mean_squared_error(y_test, y_pred_lr_scratch))\n",
    "r2_lr_scratch = r2_score(y_test, y_pred_lr_scratch)\n",
    "\n",
    "print(\"Linear Regression (From Scratch) Performance:\")\n",
    "print(f\"RMSE: {rmse_lr_scratch:.4f}\")\n",
    "print(f\"R²: {r2_lr_scratch:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
